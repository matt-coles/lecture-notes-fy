### Parallel Computing

If we are lucky enough that Sp increases with p we can make our program faster
by adding more processors, this does have an associated cost. However there are
other measures for analysing parallel programs. If we can double the speedup
with 4 processors we are doing better than if we doubled it with 8 processors.
So we can measure *efficiency* which is speedup/p, which is speedup per
processor. Usually `0 <= Ep <= 1` and is usually written as a percentage. If Ep
= 0.5 then we are using only half of the processors capabilities on our
computation, half is lost in overheads or idling. If Ep=1 this mean we are using
all the processors all the time on our computation - this very rarely/never
happens, with Ep > 1 it indicates superlinear speedup: we are using more than
100% of the processors. Efficiency is useful when we need to gauge the cost of a
parallel system: the higher the efficiency the better the utilisation of the
processors. When `Ep < 1` this indicates that somewhere at a point there is
computation cycles being lost to overhead. See slide 12 for typical efficiency
graph. 

An example of of calculating speedup and efficiency, consider pipeline:

Data moves from one processor to the next being transformed at each stage: we
can assume one time step per transform. This could equally be a CPU instruction
pipeline. A p stage pipeline will take p time steps to fill, after that it
produces on result per time step. So it can produce *n* results in `p + (n - 1)`
a sequential system will take `n*p` time steps to do the p steps on the n
computations. As time passes the number of tasks n gets large and Sp gets close
to p. As n gets large Ep also tends towards 1. Eventually we are close to using
all the processors all the time, because the only overhead is filling the pipe.

Pipelines are a really good way of making something parallel, both great speedup
and great efficiency. This is only if we can keep the pipeline full: for example
each time a CPU takes a jump, the instruction pipeline breaks and empties and
needs to refill. To keep high efficiency we need to avoid this: thus the
complications in the designs of modern processors that are aimed at keeping the
pipeline full.

Speedup and efficiency are simple but useful measures of a parallel system.
Sometimes people will use the Karp-Flatt metric as a measure of an
implementation to see how well it is doing. This is essentially a measure of the
sequential fraction of a computation (important for the Amdahl limit).

```
e = ((1/Sp) - 1/p) / (1 - (1/p))
```

A larger value for *e* indicates a larger sequential part. If you have perfect
speedup, (Sp = p) e = 0, thus perfect speedup implies no sequential part. The
reverse is true for e = 1, the program is all sequential code. 

Another measure is whether the algorithm is *work efficient* if the number of
operations it performs is no more than the sequential algorithm. For example a
parallel algorithm might duplicate some operations on separate processors as it
is more convenient or reduces communications. The parallel overhead is just
`To = pTpar - Tseq`. This measures the amount of extra work we are doing to get
the parallelism. T<sub>o</sub> can only be 0 if we have perfect speedup, which
obviously never happens.

Another question is "how scalable is the algorithm?". Here we would require a
relationship between *p* the number of processors and *n* the size of the
problem for a given efficiency.  If we increase p, how much do we have to
increase n to maintain the efficiency. Increasing p will generally decrease
efficiency (Amdahl), on the other hand increasing *n* will generally increase
efficiency (Gustafson). A poorly scalable will to increase n a lot to maintain
efficiency as we increase p. This relationship is called the isoefficiency and
expresses n as a function of p. It quantifies the balance between Amdahl and
Gustafson. Computing isoefficiency is fiddly but it is easiest to start by
looking at overhead.

```
E = 1/(1+(To/Tseq))
```

Where T<sub>o</sub> is the parallel overhead. Therefore we need to keep To/Tseq
constant. So we must of `Tseq = cTo` for some constant c.

There are many ways essentially, to measure if the parallel program is
performing well or poorly. We need to be careful that we are making meaningful
comparisons of parallel and sequential algorithms. 

Bandwidth is the number of bytes per second transmitted over some medium and
latency is the number of time it takes for the data to arrive. Bandwidth is
often mentioned in descriptions of things as it is easy to visualise, rate of
flow. However latency is often just as important in parallel systems. Bandwidths
these days are very high, Gb rates are common. Latencies of milliseconds may
seem small but relatively speaking this is a *long* time. The latency affects
programming strongly, it may be worthwhile doing duplicate computations if that
is faster than fetching a value. In large parallel systems compute power is
cheap and plentiful but communications are slow and expensive. This is why when
we implement parallel code we really need to concentrate on the communications
more than the computations. It is quite easy to increase bandwidth, doubling the
width of a bus will double the bandwidth but do nothing to the latency. We might
get huge bandwidth by strapping a USB stick to a pigeon: the latency would not
be so good, though. Moore says that sizes of RAM are doubling but speeds are
definitely not. Latency is often limited by physics: the speed of light is a big
factor on latency these days. Like Amdahls law, latency is another natural limit
on parallel computation. This problem is even more prevalent on distributed
architectures.

---

# Shared Memory Systems

Note: a single program might use several processes and a process might contain
several threads. Separate processes have separate virtual memory address spaces.
Multiple threads in the same process (generally) share the same virtual address
space. Here we consider the shared part, i.e. threads in a process.

A data race is where an unexpected or overlooked timing in the execution
produces an incorrect result. A data race is caused by any unsynchronised
concurrent access to data involving a write. Read-only data is always safe to
share: nothing can go wrong. When writes are involved things can go badly wrong.
This can even happen on a single processor when multiple threads are being
timeshared. The race may or may not happen according to all kinds of external
events, so the program can frustratingly be right and then occasionally wrong or
vice versa. 

There are myriad ways of avoiding race conditions. Some debugging tools can
detect problems like this but for more complicated cases we have to rely on
programmers thinking. You can use a programming language that prevents you from
writing code that could have data races: Rust or any functional language.

A simple solution is to allow only one thread at a time to do an update on a
shared resource.  If a second thread wishes to update while a first
has already started, the second has to wait until the first has finished. This
will ensure correct updates by avoiding the update overlap we saw earlier. This
does unfortunately introduce an inefficiency where the second thread will have
to wait. We are forcing the bits of code in the critical region into executing
sequentially which Amdahl tells us is bad. We need to make critical regions
therefore as small and fast as possible. 

One simple way of enforcing this mutual exclusion on critical regions is the use
of locks. Some people use semaphores but these are better employed for other
problems. A lock is a simple flag that says "this region is busy". We surround
all critical regions that update a given shared resource with a grab and release
of the lock. If we miss protecting any occurrence of an update the whole thing
is potentially broken.  This is clearly a good source of bugs. Often the wait on
the lock is implemented and enforced by the operating system, which might
deschedule the waiting thread to free up the CPU for something else to run, with
this kind of lock, a thread takes no CPU time while locked. Therefore the
overhead of the lock is just the CPU time it takes for the OS to deschedule and
reschedule the thread - this is not trivial. In contrast sometimes the lock wait
is implemented as a busy wait: the thread keeps trying in a tight loop to grab
the lock. The argument being that critical regions should be small to maintain
efficiency so it will only be a short amount of time before the lock is
released. And by the time the OS has descheduled the thread, the lock could
already be free so instead just keep trying, this is good for responsiveness. It
is the responsibility of the programmer to spot all uses of the shared
resources.

The problem with updates is that there is more than one operation involved,
first read, then modify, then store. Another thread may access the shared
resource in between the rad and the store. This leads us to another approach to
the update race condition by having indivisible atomic update. This is where the
hardware supplies an atomic instruction to, for example, update an integer. This
must be in the hardware the increment instruction must prevent other
modifications of that value while it is being incremented. The hardware sorts
out the sequentialisation in the case of simultaneous or near-simultaneous
update by different threads. This has limitations though, they are very hard to
build in the context of the complexity of caching and so on in modern systems,
you would need an atomic instruction for every kind of update you might want to
do, getting a compiler to generate code using that instruction might be
difficult.
