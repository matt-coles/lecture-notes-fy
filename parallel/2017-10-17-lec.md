### Parallel

# Amdahl's Law

There is a nature upper bound of Speedup `Sp <= p`. We can't get more speedup
than the number of processors we have. It turns out the number of processors is
generally not the limiting factor on speedup: there is another fundamental
restriction on speedup that is often overlooked.

**Amdahl's Law** puts a natural upper bound on the speedup that it is
theoretically possible even before we add in implementation overheads. Suppose
there is a problem of which 90% can be run in parallel leaving 10% sequential
code. For example we have to read data before we can process it: a necessary
sequentiality. Similarly for writing after processing or the add after the
square in `x^2 + 1`. In the best possible case using an unlimited number of
processors we might be able to get the parallel part to as close to zero time as
we like, the 10% sequential part still remains. Therefore measuring the speed
will be time spent on sequential processor (100/10 = 10). This would be a
speedup of only 10 even on an infinite number of processors. It doesn't even
matter what the problem is or what hardware we have.

Amdahl's Law states:

*Every program has a natural limit on the maximum speedup it can attain
regardless of the number of processors used*

We can quantify Amdahl's Law. Let T = T<sub>seq</sub> + T<sub>par</sub> be the
time spent in the sequential and parallel parts of our problem on a sequential
processor. Therefore the maximum speedup S<sub>p using p processors on the
parallel part is
```
      (Tseq + Tpar)
Sp <= -------------
      (Tseq+Tpar/p)
```
This also assumes that we have perfectly parallelised the parallel part. Thus
there is a natural upper limit on how fast programs go. Even as p->inf, you are
left with
```
        (Tseq+Tpar)
Sinf <= -----------
           Tseq
```
so there is also a limit on infinite processors. This limit is determined by the
time taken in sequential code.

```
          1
Sp <= -----------
      (x+(1-x)/p)
```
thus, `Sinf <= 1/x`.

Note that Amdahl does not say anything about how the actual speedup varies with
p: just that an upper limit exists. Your program is very unlikely to reach
anywhere close to the limit and extremely rarely in real programs actually does.
It can even decrease as p gets larger, due to increasing overheads.

Gustafson pointed out that in real life larger machines tend to attract larger
problems. Amdahl assumes a fixed size of problem.

```           
               1
Sp(n) <= -------------
         (xn+(1-xn)/p)
```
Where Sp(n) is the speedup on p processors for a problem of size n;
x<sub>n</sub> is the fraction of the computation spent sequentially. Gustafson
argues as n gets larger the sequential part relatively decreases so
x<sub>n</sub> -> 0. Thereby with p processors on an infinitely sized problem:
`Sp(inf) <= p`.

Both Amdahl and Gustafson are correct, they just apply to different cases of
scaling. This should convince you that even deciding what to measure is
problematic. It does re-emphasise the fact that parallelism is not about making
this faster but making things larger. 

Speedup is a simple measure often proving that your parallel program is slower
than it ought to be. Sometimes it takes p to be surprisingly large before you
even catch up with uniprocessor time (sometimes never!). It is also possible
that when adding more processors you can make the problem slower than if you'd
had fewer. This is usually due to increased communications between the
processors adding more overhead but not more speedup, perhaps due to Amdahl. Of
course it's not always this bad but it's quite common. It does mean that there
is often an optimum number of processors for a given size of problem that
achieves the best speedup.

There is a small class of problems called *superlinear* problems that can run at
`Sp > p`. This can happen for a variety of reasons, some technological and some
more philosophical. The first technological reason is due to cache memory, if
you can fit your entire problem inside the cache it will run faster. Therefore
with *p* processors you have *p* times as much cache, so the problem might now
fit entirely in the extra cache where it didn't on a uniprocessor. This does
take a certain kind of low-communication, easily divisible problem to work.
Amdahl's doesn't apply here as we are not comparing the same machine with an
identical parallel system, there is much more cache.

Another reason that speedup can go greater than *p* is due to the way speedup is
defined (see previous lecture). For example, take bubblesort running on a
uniprocessor and we wish to make it run on a parallel machine. One way of doing
this is split the data into equal halves bubble sort each half in parallel and
merge the two sorted lists together. (2-way parallelism), the middle step can be
parallelised recursively as deeply as you want, given any number of processors.
Seems like a reasonable to implement bubblesort on a parallel machine. To
calculate speedup you need to time how long it takes to on each system.
Calculating complexity will see that this gives p<sup>2</sup> speedup! Sometimes
by changing the algorithm to run in parallel we actually improve the algorithm
and it would give speedup even on a uniprocessor system, so it's not really a
superlinear algorithm as the speedup is not comparing like with like. This is
fairly rare but is worth being aware of the possibility. You can potentially
define speedup as the time of the best possible sequential algorithm over time
on *p* processors, but this has it's own problems because we don't always know
the best possible sequential algorithm.

Similarly you can get superlinear speedup if that original sequential program
was poorly implemented. When more thought is put into the parallel version it
becomes substantially better than the sequential version. One final cause of
superlinear speedup can be due to randomness, if the data contains random
numbers or there is something that adds elements of randomness, you can get
superlinear speedup if the parallel version 'gets lucky' and finishes earlier
relative.

The conclusion is that speedup is a nice and simple way of measuring advantages
of parallelism but it is important to take care over the data. Some programs are
pathologically parallel meaning they fall easily into parallel parts that have a
minimum communication and we can easily get good speedups. e.g. graphics
rendering, weather forcasting, parameter sweeping and so on. Other problems fare
less well.
