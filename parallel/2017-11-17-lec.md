# Parallel

### Distributed Memory

We could use interfaces like threads or OpenMP and have an underlying or
virtualising infrastructure that converts them to message passing between
processors over a network. Most people don't like that as it hides the source of
the cost of distributed parallelism from the programmer making it harder to
design and write efficient programs. So most distributed programs are explicitly
message passing, or have some other way of making the cost of an operation more
clear.

A big player in this field is Message Passing Interface (MPI). Among others
there are PVM (Parallel Virtual Machine) etc. MPI is what Big Science uses, when
terabytes of data crunching is needed. Distributed systems are not good for
small programs due to the overheads. MPI runs the same program on multiple
processors (SPMD) but definitely not in lockstep. MPI is simply a library of
functions to do messaging, you can use it with normal C. Code written to the
standard should run on any implementation but frequently doesn't.
