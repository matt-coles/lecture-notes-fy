### Parallel

When using distributed memory you try to keep the data a process needs on the
processor it is running on, maybe even replicating data or computations to avoid
accessing remote data as this is **very** slow. This is an ideal that is rarely
achieved in real programs. Replicating data and computations is problematic as
not only is the coherence problem relevant again, it is now the programmers job
to deal with it. This can be resolved by replicating read only data, such as a
table of values but this still has performance implications for the read-write
data. More sophisticated systems have extensive hardware support for messaging,
they have DMA hardware that can access memory independently of the CPUs.
Therefore you can use these systems to do asynchronous message passing, freeing
the CPU to do something else while the message is being processed by the DMA
hardware. This allows more computation but at the cost of complicating your
programming. The idea of overlapping computation and communication will reappear
many times in parallel processing.

In distributed systems the concept of single shared values has to go completely
out of the window. Programs have to be written with this in mind, global shared
mutable values are simply not a good idea (even in uniprocessor programs!).
Distributed memory as an architecture is very popular and used by clusters: each
node is effectively a simple computer. This is very suitable for MIMD, typically
used as SPMD but is not effective for SIMD processing. Even with the huge
message passing overhead, clusters are widely used particularly with very large
problems where the overhead is small relative to the rest of the computation.
The computation do need to be very large to overcome these overheads - it is not
at all suitable for small problems or problems where data needs to move a lot
between processors. This sort of architecture scales very well and can support
clusters of well over a million cores. When you add a machine to a shared memory
architecture, there is a large cost due to the complicated memory architecture.
In contrast, the cost of adding CPUs to a distributed memory system is simply
the cost of the CPUs plus a tiny bit of networking. This is fairly linear
scaling but you do need to scale the network otherwise you get the same
bottleneck as in a shared memory system.

One way to scale this network is to use a 'fat' tree, where the links nearer the
root of the tree have larger bandwidth thus allowing full simultaneous bandwidth
between each pair however there is a lot of latency between each node. In
addition creating a fat tree is very expensive. In addition you cannot always
afford to build a 'full fat' tree so you use a lighter alternative, trading
bandwidth for cost. Many other network topologies exist such as hypercubes and
torus etc. The takeaway here is that it is cheaper to expand a distributed
system than it is to expand a shared memory system. However decreasing latency
is very expensive whatever the system.

Some programmers don't like the fact that distributed memory machines require
programming using message passing and prefer the shared address space model;
shared memory is easier to write programs for (they claim). They can use virtual
memory as a way of converting shared virtual memory addresses into distributed
memory addresses. This is sometimes called distributed virtual memory or
distributed shared memory. 

Reading and writing data will be very NUMA and probably implemented by a message
passing layer hidden from the programmer. Unfortunately the programmer does need
to care about the location of the program as otherwise the speed will be very
hard to predict or control depending on how data is distributed across memory.

In the operating system Mosix, the underlying communications were implemented
for you so everything *just worked*. Or by contrast they can be done in the
programming language or libraries such as Cluster OpenMP or Unified Parallel C
(see later), so the language may need a little learning. However virtual shared
memory as a whole is fairly rare.

---

The next architecture has elements of both shared and distributed memory, a
vector processor, which is a SIMD collection of CPUs (actually just ALUs) often
with a chunk of global shared memory and a single control unit. Each processor
also has it's own chunk of local memory that it operates on, as this is data
parallel. This is not a cache, just a per ALU chunk of memory. Cache memory is a
very fast local copy of a slower memory location which needs to be coherent with
other cores, whereas with the ALU local memory is per core, not necessarily fast
and we expect different values for different locations in each core.

Hardware coalescence can make processors faster but only if the programs are
written in a particular way. Similar for writes, ALU *k* writing a value to the
*k*th slot in an array, multiple writes to a single location make no sense and are
often disallowed by the system. Often there is fast direct communications
between neighbouring CPUs. This allows data to be shuffled up and down the
vector very quickly: many problems, for example differential equation solving
work on data and neighbouring data in this way. Vectors are only SIMD and not at
all suitable for MIMD or even SPMD. Vector processors appeared early in parallel
computing as they are relatively easy to build ALUs are relatively easy to build
compared to control units. An extension of the idea was the array processor,
which is ALUs connected in an array rather than a vector. The CPUs would still
be in SIMD lockstep. Additionally you can have diagonal connections to the
processors. This fits well with 2 dimensional differential equation problems.
More expensive and much less common.

---

A similar architecture is something called a systolic array. These generalise
instruction pipelines to processes.

`-> CPU mem -> CPU mem -> CPU mem ->`

The CPUs are independent(MIMD/MPMD), each performing one step in the
transformation of the input data. This is more often found in hardware to solve
specific problems, not often found as generic machine. For example a graphics
card might want to do clipping of polygons, then colouring then shading. Each
step is separate but each is compute intensive so it makes sense to split it up
into pipelines. The downside of pipelines is that there is latency, see digital
TV for a real world example of this. Systolic arrays are the obvious extension
but examples are rare to non-existent.

---

There are such a number of different architectures because there is no model,
like the von Neumann model that encapsulates multiprocessors. There are many
contenders but there are no obvious winners. There are many theoretical models
whose aim is to guide the design of parallel algorithms and allow the analysis
of them. As with von Neumann the idea is such that you can write your program in
accordance with the model and the model maps well onto all kinds of real
hardware then your program will map well onto all kinds of real hardware.

One such model is called PRAM, or the Parallel Random Access Machine model, this
idealises a parallel computer as shared memory MIMD concentrating on the memory
bottleneck. It looks at how one accesses memory in this kind of machine.

- Exclusive Read, Exclusive Write (EREW). Each memory location can only be read
    or written by one processor at a time. The simplest architecture.
- Concurrent Read, Exclusive Write (CREW). Each memory location can be read by
    many processors simultaneously but written by only one at a time.
- Concurrent Read, Concurrent Write (CRCW). Each memory location can be read or
    written by many processors simultaneously. Not a realistic model.
- Exclusive Read, Concurrent Write (ERCW). Never used.

PRAMs make further simplifying assumptions including:

- Memory is symmetric: every location is accessed at the same speed
    (decreasingly realistic)
- There are an unlimited number of processor: if you need another core then
    there is one to use. Seems unrealistic but not as bad as it seems as most
    programs already cannot make use of all the hardware available.
- Memory is unlimited. This assumption is also often made in analysis of
    uniprocessor algorithms.

In the early days of CS, there were many clever algorithms to deal with the lack
of available memory however this is not such a large problem any more even
though memory has got bigger so have the problems we want to solve. So you
analyse your program counting the number of memory accesses according to
(EREW/CREW/CRCW) and this gives you a measure of time your program will take to
run. This is primarily a MIMD model and you can analyse SIMD using it. It
totally ignores important realities like NUMA and other overheads such as
communication time in a distributed memory system.

There are many other models, such as BSP, or Bulk Synchronous Parallel which
tries to take communication time into account. It assumes processors with local
memory communicating over a network. This is good for distributed but also for
shared memory where you just have smaller communication costs. These analyses
are hard to do but are a better match to reality. 

---

There needs to therefore be a way to analyse parallel algorithms. This is
similar to analysis of sequential algorithms, just more complicated. This leads
to statements like "this takes time O(n<sup>2</sup>) using O(p) processors. They
mostly measure the parallel algorithm in comparison with a corresponding
sequential algorithm.

One algorithm is called **speedup**. We have seen that having *p* processors
won't necessarily make our program run times *p* times as fast.
Speedup<sub>p</sub> therefore equals time spent on sequential divided by time
spent on *p* parallel processors. Ideally we'd like S<sub>p</sub> = p but this
never happens. Usually it is much smaller for several reasons. Firstly there is
communications overheads, this might be small for shared memory or large for
distributed memory but it is still present. Speedups on distributed memory
machines can be reduced as overheads are high, but they do improve as the
problem gets larger and the relative cost of communications drops. Remember
clusters are used for large problems where the emphasis is on size not speed. In
really bad cases, S<sub>p</sub> can be less than 1, i.e. parallel program goes
slower than the sequential program, this is more common than you'd think.
