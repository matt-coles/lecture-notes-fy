### Parallel Computing

Moore's Law is a law about the number of transistors in a chip, and this law states that the number doubles every two years. He did not say that speed doubles - this is a common misquote and there are some variants, which says 18 months rather than two years. Profit margins on silicon wafers mean that it is better to have fewer larger chips than lots of smaller chips. Whilst the speed of processors has started to level off, the number of transistors is still increasing. This has led to people producing multicore processors. This new computational power is only useful if we can write software that exploits those multiple cores.

In addition to processors, Moore's Law applies to memory chips, in that they are doubling in capacity at a similar, potentially even faster, rate. Another law, Koomey's Law observes that a given amount of computation has taken less and less energy as time passes. Again, around double per 18 months, this is very important as battery powered systems have become much more common recently.

The death of Moore's Law has been predicted for many years but has not come yet. It must come to an end at some point but we don't know when, technology has thus far kept improving the transistor count.

In the very early supercomputers there was a common kind of hardware called vector processors, this is for data parallelism, namely scaling the data not necessarily the speed. For example add together these 100 pairs of numbers to produce 100 results. **See MSA spec for MIPS for an implementation of vector processing**. A vector processor is a collection of multiple ALU's and they are not independent of each other, they all do the same operation at the same time, this is because they can only fetch one instruction however they can do this on different data. This is called SIMD.

Another architecture for parallel machines is a cluster of computers. This is essentialyl a large number of normal computers all connected together in a network. The program can then be spread across all the nodes. This allows you to do both process and data parallelism. The hardware is fairly cheap so clusters with thousands of CPUs are common, there are economies of scale such that clusters with millions of cores exist. 

Moving data is so much slower than simply doing instructions. Transferring over a network is measured in milliseconds whereas CPUs are on the nanosecond scale, this means that just having a massively parallel machine does not mean that it is faster to use it in a parallel way. It may be faster to recompute something than actually fetch it from memory, this is very different from sequential programming.

# Types of Parallelism

**Bit level**

Consider an adder:
A serial adder works on bit at a time, propagating the carry up each time. A parallel adder work on bits in parallel and then deal with the carries afterwards, this is more complex hardware but is faster, this allows to add large numbers in single CPU cycles. This is an example of how parallelism trades complexity for speed.

**Pipelines**

Instructions are executed faster by using pipelines. This is parallelism by overlapping the fetch->decode->fetch arguments->execute->store result. **See first chapter of See MIPS Run about fish & chip shop**. It also shows how clock speed is not always a good measure of speed.

**Coprocessors**

Early chips were too small, so operations could be offloaded to a separate chip, a coprocessor. **See MIPS FPU which lives in Cu1**. This is a weak form of parallelism because the main processor can continue with other operations, but it will still need to wait for the result of the floating point operation to do anything with it. Another example of a coprocessor is a GPU, originally specialised for pixel crunching but now are suited for many more operations, this is called GPGPU (general purpose GPU).

**Superscalar**

Putting multiple ALUs on the chip, this could for example be two add units. This means that the processor can now do multiple instructions at one time. This means if you have the code:
```
x1 = y1 + z1
x2 = y2 + z2
```
The independent adds can be done at the same time, i.e. in one CPU cycle. However if the second add required the result from the first add, then they cannot be done at the same time. The CPU must have a dependency checker to determine if it can parallelise the instructions.

There can also be out of order processing that schedules an instructions in such a way that it reorders the instructions for independent computations so that it can utilise it's superscalar parallelism. There is also a role for compiler writers that can make the instructions easier for the CPU to analyse.

**Hyperthreading**

The next stage is to duplicate the state bearing parts, i.e. the program counter and the registers. This allows two, sometimes more threads of instructions to share the available hardware. There will be conflicts between the threads if they both try to use a computational unit when there is only one unit of that type. In that case on thread will have to wait. This makes things look like a multicore system but is not truly multicore. It all depends on the amount of replication in the architecture as to how much parallelism can be gained. 

**SIMD**

The idea of vector processing has been adopted in the instruction sets of some processors. **MSA in MIPS**. It arose from multimedia processing, graphics in particular. This means that we can regard a 64 bit register as both a 64 bit register, two 32 bit registers, four 16 bit registers and so on. For example, in the MIPS SIMD Architecture it shares the 128 bit floating point unit registers. This is sometimes referred to as SWAR (SIMD within a register). This was found to be very effective for data parallel graphics processing. This does require compiler support to generate these kind of instructions. It is not always possible to vectorise floating point numbers unless given explicit permission because it is not associative.

**VLIW**

The transisition of CPUs from CISC to RISC (MIPS!!) was based on advances in compiler technology. The idea was to move complexity from hardware to software, rather than using complex instructions poorly and uses simple instructions effectively. This is reliant on the compiler being able to exploit the details of the RISC architecture. The same idea was touted for *very long instruction word*. This was to design a processor with many repeated arithmetic units, with very long instructions, for example up to 128 bits, this would allow for composites of simple operations. The compiler can then compose these instructions and make sure there are no conflicts involved. This is building on the idea of shifting complexity from hardware to software, so as not to waste runtime at the cost of compile time. This reduces power usage as well. This has been tried multiple times but is not very popular as compilers are unable to do the analysis efficiently to make good use of the hardware. Intel did try to revive this with their Itanium line of processors in 2001, however they called *Explicitly Parallel Instruction Computing* (EPIC), which is very similar to VLIW, this too failed. Probably due to x86 being so strongly entrenched and still that compilers can't quite manage it.

**Multicore**

This is a true multicore system which has full replication of arithmetic units, control and registers. At no point does one core have to wait for another to finish what it is doing. This is two or more full CPUs on the same chip. Early multicore machines were unicore chips side by side on the same motherboard. We can now put them on the same chip and can share a cache(L2) and other chip infrastructure. This means that there is much faster inter-processor communication. Off-chip transfers run at bus speed which can be much slower than the chip speed. Large machines can be multiple multicore machines. Balena has two 8 core chips on a motherboard, a total of 16 threads of execution (32 with 2-way hyperthreading). This is slightly asymmetric, in that some processors are closer than others, so some communications can take longer than you usual. 

Of course all of these techniques are not mutually exclusive and can all be used together. A typical large installation is a CLUMP. This consists of a cluster containing multiple processors, with multiple cores, with hyperthreads, SWAR instructions, pipelined architecture, parallel instructions and coprocessors. It is very difficult to make efficient usage of all of those techniques at one time.
