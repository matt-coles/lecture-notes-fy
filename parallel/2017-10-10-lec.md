### Parallel Computing

With all these kinds of parallelism we need some means of classification. The
simplest classifcation was devised by Flynn in 1966.
 
  - Single instruction, single data (SISD). Traditional, von Neumann,
  - uniprocesor machines Single instruction, multiple data (SIMD). As in a
  - vector processor, all processors doing the same operation on different data.
  - Multiple instruction, multiple data (MIMD). Multiple processors doing
  - different things to different data - this is what most people think
  - parallelism is about (it is not) Multiple instruction, single data. (MISD).
  - This is sometimes interpreted as redundancy, multiple different computers
  - all processing the same data.

People have extended Flynn's classification, by subdividing the MIMD section.

  - SPMD. This is a MIMD machine that has all cores running the same program, on
  - different data but they are all going their own way on loops and
  - conditionals as opposed to SIMD where all processors are executing the same
  - instructions at the same time (in *lockstep*).  MPMD. This is a MIMD machine
  - that is running different programs, eg master-slave models or systolic
  - pipelines.

There are many more classifications, in particular the architecture of a given
machine. A known architecture would be the von Neumann uniprocessor model.
Additionally mentioned before there is a coprocessor which is a slave to the
main processor, this is currently very popular in the form of a graphics card -
this can be connected to the CPU or memory or both. A mulitprocessor is a loose
term applying to most parallel architectures except possibly SIMD (which usually
doesn't have multiple full processors). A multiprocessor has shared memory when
the processors access memory on a shared bus. Processors share each other's
data, if one processor modifies the value of a variable, the other processors
see that change. Shared memory systems have a bottleneck at the memory bus, as
memories and buses are slow relative to a processor. This is usually solved by
having a cache between the processor and the main memory.

This introduces the problem of cache coherence, the global copy is out of date.
If another processor wants to read the value before the updated version has been
written back it will get the old value. Dependent on timing you don't know if
the CPU has written the value back and you don't know what you are going to get.
This is an example of a race condition, a data race. The cache coherence problem
is trying to ensure that all cached copies are the same. There is a variety of
solutions, you can use a writethrough cache that writes the cached value back to
main memory as soon as you write to the cache. The other caches are watching and
will grab the updates as they need them, this is expensive hardware and does not
scale well. This is not as bad as it sounds though because typical code reads
much more than it writes, this makes cache snarfing more efficient than one
would imagine. 

Another solution is to have very fast main memory, but this is not really a
solution as it is expensive or alternative clock the processor down to the
memory bus speed. This was tried by IBM and it worked fairly well, however it
was not marketable.

The bottom line is essentially that symmetric shared memory does not scale well
without very complex hardware support, the limit is around 100 cores. The intel
Xeon Phi has 72 cores with sophisticated cache coherence and a fast ring bus
connecting processors to each other and to memory. It is sufficient for small
machines use, with around 16 cores. It is well suited to MIMD although note that
SIMD also uses symmetric shared memory. 

We can also structure our memory in a different way, for example a tree where
each processor has different access times to different areas of memory. There
are various other topologies, stars, rings hypercubes and full interconnect etc.
This is called Non-Uniform Memory Access or NUMA. This scales much better than
symmetric share memory but now programs have to take into account data locality.
The data that a processor needs should be kept as close as possible. It can make
a huge difference to the speed of a program if the data is not where it should
be. NUMA implementations will stratify the memory in terms of "distance". This
can be simplified to local, remote and 'far away'. This also means that the
program or OS must have knowledge of the NUMA hierachy and some level of
knowledge of the programs needs. This is still a matter of great research and
development.

Additionally there are hybrid systems where some processors share memory
symmetrically and some have NUMA.

An example of a NUMA architecture is SGI's 2048 core shared memory machine
running a standard Linux distribution using an unmodified kernel. They claimed
the hardware scaled to 262144 cores sharing 16TB of memory. This was called
ccNUMA or cache coherent NUMA. The memory could be connecting in a number of
ways, such as a fat tree or a torus with a claim of a maximum 2 microsecond
latency.

It is also possible to remove the problem of memory bottleneck and cache
coherence just by not having shared copies at all. **Distributed memory** says
that each processor's memory is its own and is entirely separate from every
other processor's memory. In contrast to a shared memory processor system where
the address space is the same for all machines, in a distributed architecture,
each processor has it's own address space and each address can refer to a
different value or location. Typically in such a system the processors are
joined by a network. To get at someone elses data, a message must be sent over
the network which will then respond with the data. This message passing is much
slower than simple shared memory accesses so such accesses should be kept to a
minimum and the position of data is **very** important. Note that it is stil
possible to use message passing on a shared memory architecture, you might do so
for program structure reasons.
