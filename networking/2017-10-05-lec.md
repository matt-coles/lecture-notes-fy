### Networking Lecture 05/10

In a network, redundancy is important, having multiple paths between points introduces reliability. The internet is a hierachy that is managed by multiple different organisations at each level of hierachy, each level builds on the previous one. Specifically the Internet is the worldwide collection of networks, the distinction is made from an internet. **the Web != the Internet**. The Web is an application that runs on top of the infrastructure that is the Internet.

The basis of the Internet is a collaboration between all of the member hosts, data travels by being passed from host to host. This can be illustrated by the `traceroute` linux utility.

A brief history of the collaborative nature of the Internet goes like this:

  - 1958: the Russians launch Sputnik
  - mid 1960s: ARPA is formed which is a project to share expensive resources, primarily computers
  - The network that ARPA produced was decentralised and did not have single points of failure - this was to avoid nuclear attacks from Russia
  - An additional requirement was for additional paths between hosts
  - Using simple circuits - like the telephone system that was used - was deemed to vulnerable, a mechanism called packet switching was devised

Packet switching is a mechanism whereby the data is chopped up into individual chunks called packets and they can all be sent individually, potentially over different routes. This means that through the magic of TCP if a packet is lost or something breaks, the packets can be resent. It is then the responsibility of the receiving host to reconstruct the data once it has all arrived.

More timeline:

  - 1969: the first Internet has just four nodes, runs NCP (network control program)
  - Email and discussion groups prove immediately popular
  - 1973: the Internet reaches London via satellite link
  - 1974: TCP/IP replaces NCP, this technology is what we all use today
  - 1980s: Milestone for the Internet reaches 1000s of machines
  - Following this the Domain Name System arrives to aid discovery and naming of machines
  - 1980-90: ARPANET is decomissioned and replaced
  - Other networks based on other protocols begin to be replaced by the Internet
  - In 1992 there are 1,000,000 hosts
  - Gopher arrives - this still exists today!
  - Tim Berners-Lee invents the Web.
  - This means that now the Internet can enter homes
  - Microsoft gives up on it's own network and falls into line
  - the Dot Com boom, a lot of money was made on the Internet
  - the Dot Com crash, a lot of money was lost on the Internet
  - Broadband comes to homes, speeds increase and this begets the arrival of large commerce
  - Then begins the 'mobile revolution' which takes us up until today

The decentralised and packet nature has implications on how the Internet *must* work. Potential problems are to do with how to chunk the data, calculating the route which a packet will take and how we can reconstruct that data when it arrives at the destination. Additionally decisions must be made about the hardware on which to run the internet. Any individual packet does not know how to get to it's destination, in fact even the source host doesn't usually know the entire route to the destination - unless the destination is on the local network. This can be modelled like a postcard with an address and this relies on the routers (postman) which passes on the data to the next 'hop'.

# Part 2

To ensure interoperability the Internet relies on standards and standardised protocols, this is so that all machines can all communicate together without friction. This means that any two machines can communicate without any prior interaction. The Internet specifications are kept in a collection called the *Request for Comments*, or RFCs. They are freely available on the Internet. They attempt to follow a philosophy, be strict in what you do yourself and be liberal in what you accept from others. Additionally there are several other bodies that oversee the structure and working of the Internet:

  - the ISOC (Internet Society): Oversees the Internet standard development processs
  - Internet Architecture Board (IAB); ISOC commitee that oversees technical and engineering development of the Internet, particularly IETF and IRTF
  - Internet Engineering Task Force (IETF); IAB committee that develops standards and publishes RFCs
  - ** See slides 45-47 of Networks Protocols for complete lists**

IANA delegates responsibility of various things to Regional Internet Registries (RIRs), current RIRs are AfriNIC, ARIN, APNIC, LACNIC, RIPE. They then further delegate management of domain names to commerical companies. Other important bodies include:

  - IEEE, manages hardware like Ethernet and Wi-Fi
  - ISO, which are responsible for XML standards
  - IEC, responsible for other standards e.g. DLNA
  - ITU-T Standards section of the ITU, manages things like DSL standards
  - And many more

There are a lot of things that need standardising, and a lot of people who want to implement their particular standards, hence why there are so many standard bodies.

The primary standard that is used on the Internet is Transmission Control Protocol/Internet Protocol or TCP/IP. The name is historical rather than accurate because we need to first identify the layers that build up the standard.

There are two main models in use, these are the ISO OSI Seven-Layer Model and the Internet Four-Layer Model. The OSI model is widely used in networking courses and the Internet model is the one that is actually used in the real world. 

---

The Seven OSI Layers are:

  - Physical
  - Data Link
  - Network
  - Transport
  - Session
  - Presentation
  - Application

The physical (PHY) layer is the hardware layer and deals with the transmission of bits over a channel. This encompasses things such as what's voltage to use, or colours of light pulses or wavelengths, what encoding for bits, how many wires to use in a cable, what plugs etc

The data link layer or media access layer, takes the physical layer and tries to create a channel where there are no undetected errors of transmission. Networks are not 100% reliable so we must take into account possible errors and the ISO standard recommends that this layer is where you must think about this. A typical MAC layer will send the data as a sequence of frames. A frame is a chunk of bytes, maybe tens or thousands of bytes long, the point is it is variable. If a frame is corrupted the MAC layer can attempt to resend it or let the next layer know that there is a problem or more frequently, do nothing.

The network layer controls the operation of the network particularly the issue of routing data from source to destination. Also it can deal with congestion where there is too much data, it can solve this problem by rerouting data or using flow control to slow down the rate of transmission, or speed things up. Accounting can be thought about in this layer so that we can count the number of bits in order to bill the user at a later date - however this rarely happens in the real world. Finally you should also think about QoS in this layer, ensuring that there is always enough bandwidth to stream a video.

The transport layer accepts data from the session layer and arranges it into packets suitable for the network layer - this is called packetisation. Similarly it takes packets from the network and reassembles it, this is called depacketisation and also needs to deal with packets arriving out of order. It is also recommended to think about reliability in this layer so that there is no corruption or loss in the data - this is a small amount of overlap with the data link layer. Reliability is not always a requirement of a network though.

The session layer manages sessions between source and destination. This is a bit of a fuzzy concept but largely encompasses establishing and terminating connections, for example a remote login session. It should be able to restart interrupted sessions. A session can be of any length, for example, one email being sent, in short it is just a logically connected set of exchanges that have some unified identity. For example if the network goes down half way through a large data transfer the sesion can be easily picked up again.
