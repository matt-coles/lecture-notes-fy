### Compilers

The steps of building an interpreter might be as follows:

  - Define semantics of each language term, for example an if statement or a
  - while loop.  Identification of auxiliary semantic structures, these are the
  - structures for keeping track of what's going on. One of these is usually
  - called the 'environment', a mapping between variable names and variable
  - values, in addition we must keep track of closures for the --C language, but
  - this will not be present in all languages. Subpoint: a closure is just a
  - function pointer with an attached environment. A further structure is a
  - run-time value representation, such as type information etc. The fourth kind
  - of structure is a representation of control flow, initially we can use the
  - control structures of the language we are writing the interpreter in - at
  - some point we may need to do things that are more exotic than are present in
  - the host language.  Define a procedure to traverse the AST. 
  - `interpret = term * environment -> value`

Initially the environment is essentially is something that maps a name to value,
or potentially a location if we need to provide updates. Considering the names
`f(x+2)` this contains both the values `x` and the function or closure `f`. Note
that a function is just a closure with a null environment. One potential
solution is substitution, where we would reconstruct the by substituting in the
values of `f` and `x`, this is similar to carrying out beta-reduction. This is
expensive in both space and time, this is why we use the environment model in
most conventional languages. In the environment model it is required to map a
name to a location. This also means that we must consider both the lifetime of
our mappings and scope (global/lexical/local/dynamic etc).

A simple interpreter solution would consist of:

  - Sequence of frames, this could be a list or a stack for example Contain a
  - name/value mappings

A compiler solution would need to index into those frames, with a compile-time
map. This means that there will be a map saying where a variable might reside
relevant to the stack pointer. Compilation can be considered as a process that
discards as much unnecessary information to run faster but still correctly.

More formally a closure can be defined as `cl = environment x variable* x term`.
Where variable* is a pointer to the argument list and term is a pointer to the
body. This is a representation of a procedure or function. This kind of
structure is only really necessary in a non-global scope because we need to know
which environment to find the values of the free variables within the function.

# Environments

 - *R(x)* the value of *x* *L(x)* the address of *x* This is Strachey's names,
 - locations and values model.

This means that names map to locations and then finally those locations refer to
a value. Interpreting a name in the symbol table means finding it's associated
*R* value in the environment. Assignment is a similar process but it means
finding the right place int he list and then updating the next element.

# Scoping

Haskell and OCaml are examples of languages with lexical scope. This results in
a broad shallow tree for environments and whilst the access cost is O(n) but n
is typicall small compared to dynamic scope variable access. This is in contrast
to dynamic scope, where you extend the scope of the caller rather than the
callee. This results in a narrow deep tree however n is usually much larger.

# Calling conventions

  - Call by value: This is called eager evaluation or applicative order
  - evaluation. Example: if you call f(a+b), definition f(x), x gets value of
  - a+b.  Call by reference: If you call f(y), definition f(x), x gets the
  - address of location y.  Call by value result: This is similar to call by
  - value, but locations of actuals updated before return. In effect is like
  - reference but because updates within the procedure are local is more
  - efficient than by reference.  Call by name: If you call f(a+b), definition
  - f(x), x gets a procedure of no arguments such that every time x is used in
  - the body, the procedure gets called. If a is global, it could change between
  - calls, this is called Jensen's device.  Call by need: This is similar to
  - call-by-name, however the procedure of no arguments will be called only one
  - time and then the value is saved and is returned for each subsequent
  - reference to x. Or called laxy evaluation or normal-order evaluation.

## Value Representation

For value representation we need to categorise both the purpose, and the place
or duration. For purposes, there are two potential purposes, one being language
implementation structures - which are implicit, for example the call stack, and
another being user program structures - which are explicit, for example
`malloc`. For it's place or duration, there are static storages, which are
in-code, a fixed size and exist indefinitely, this is present in C and FORTRAN.
Automatic storage, which is on the stack, variable size, has a dynamic extent
LIFO. Dynamic storage is on the heap, has a variable size and has an indefinite
extent.

In order to represent values we must first decide upon the set of computable
values in the language, for our case we will have integers and closures, but
this could be many different types. Type policy must also be decided and this
could be declared - no runtime checks, inferred, which is present in Haskell
where there is no or limited run time type checking or latent(dynamic) which is
run time only type checking.

We can do type checking by just having a big bag of pages where there are
multiple heaps with a few fixed types. The type check oepration is therefore
`lower <= ptr <= upper`. This is messy and variable rates of consumption make
this difficult.

A simpler solution is to have a numeric code associated with each object which
has an overhead. Then we can just dereference the pointer and find it's type
information. This means a typecheck costs a memory access, and uses a large
constant and each object needs an additional word of storage. It is incompatible
with foreign functions, i.e. libraries written in other languages.

This can be improved by using a "tagging" system, this is optimisation of boxing
for the most common types. We can then make a choice of whether those bits
should be in the high bits or low bits, this is tradeoff between portability and
performance. At the cost of addressability you can store the type information in
the top bits of the address, in addition this requires us to load large
constants and also limits the number of potential tags. 

We can use the low 2/3 bits because all the objects we are allocating must be
word or dword aligned, this gives a very limited number of tags unfortunately.

We must also store "stack frames" however it is not always practical to allocate
them on the stack. They have typicall four parts, frame data, arguments, locals
and temporaries. Allocating on the heap instead of the stack supports access to
non-local lexical bindings.

## Storage Management

Three policies:

**No deallocation** The application handles it: implementation of implicit or
explicit deallocation in application by the user.

**Explicit deallocation** For example, malloc/free etc, this leads to dangling
pointers, memory leaks and is very difficult to debug.

**Implicit deallocation** Garbage collection: prove object is not reachable and
then allow it to be deallocated.

# Garbage Collection

Intrusive: stop computation, recover memory and restart.

Real-time: interleave computation and recovery

In order to prove that a memory is no longer used there are one of two methods:
reference counts OR tracing + marking. To then reuse this memory, the options
are essentially to have a free list, or compaction. A free list maintains a list
of memory chunks that are available to reuse, or you can squish your memory
chunks together to get a contiguous chunk to begin allocation from. 

One can define accessible data as data that can be reached by a path from a root
(pointer in the stack or global variables). The task is therefore to identify
all the locations of memory in the heap that can't be referenced.

Reference counting is simply keeping count of the number of pointers to an
object. On creation of an object there will be an object with `count == 1` as we
have just created the first pointer to it. Each time a new reference is created
you increment the count, and decrement when references are destroyed. Once the
count is zero add the chunk of memory to a free list - this is not feasibly
compatible with compaction, you can also have multiple free lists of different
sizes to speed reallocation. The pros of using this is that there is immediate
recovery of memory and is a simple implementation, however it causes
fragmentation, overheads and cyclic structures are not recovered as they will
never reduce each others reference count.

Stop + Copy / Generational is another mechanism of garbage collection. Live data
is copied and compacted in one step. Memory is divided into 2 equal regions
(FROM+TO). We can allocate memory with an incrementing pointer in the FROM
region. When the pointer reaches the end of the FROM region, we can copy every
bit of reachable memory(from roots) into the TO memory region. At this point you
flip the FROM and TO regions conceptually, then the continue allocating and
start the process again. This is advantageous because it allows us to recover
cyclic structures, compacts live data and only ever scans live data. The cons
are that this wastes memory, with the division of memory and is intrusive. There
is a variant which runs in real time called Bakers real time data. NB: To
prevent repeated copying of shared pointers you must leave a 'note' to say that
something has already been copied.
